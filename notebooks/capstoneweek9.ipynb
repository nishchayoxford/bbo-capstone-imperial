{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8caefdb-699b-4ce1-834d-575a6d5f98c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== WEEK 9 QUERIES (COPY/PASTE INTO PORTAL) ====\n",
      "\n",
      "F1  [MINIMAL_IGNORE]\n",
      "  Week 9 query: 0.005626-0.830621\n",
      "\n",
      "F2  [RIDGE_MICRO]\n",
      "  Week 9 query: 0.722773-0.263092\n",
      "  tuned alpha: 0.0001 | LOOCV MSE: 0.697657 | tuned step: 0.002\n",
      "  best observed y: 0.627259 at x_best=0.724285-0.264402\n",
      "\n",
      "F3  [RIDGE_MICRO]\n",
      "  Week 9 query: 0.179573-0.365715-0.880210\n",
      "  tuned alpha: 1.0 | LOOCV MSE: 1.349532 | tuned step: 0.0065\n",
      "  best observed y: -0.047393 at x_best=0.178771-0.372140-0.880781\n",
      "\n",
      "F4  [RIDGE_MICRO]\n",
      "  Week 9 query: 0.787305-0.271802-0.369425-0.652092\n",
      "  tuned alpha: 1.0 | LOOCV MSE: 1.557988 | tuned step: 0.0045\n",
      "  best observed y: -12.699964 at x_best=0.785000-0.275000-0.370000-0.650000\n",
      "\n",
      "F5  [TRUST_REGION_BO_MICRO]\n",
      "  Week 9 query: 0.756823-0.306295-0.854453-0.799818\n",
      "  GP kernel: 11.9**2 * Matern(length_scale=[0.4, 100, 100, 1.64], nu=2.5) + WhiteKernel(noise_level=1e-12)\n",
      "  local-train K: 6 | xi: 1e-08\n",
      "  local_sigma: 0.0045 | radius: 0.01125\n",
      "  candidates kept: 33292 | best EI: 3.318471e+01\n",
      "  best observed y: 413.127892 at x_best=0.745989-0.305287-0.849251-0.788893\n",
      "\n",
      "F6  [MANUAL_MICRO]\n",
      "  Week 9 query: 0.238844-0.691156-0.368844-0.775260-0.435578\n",
      "\n",
      "F7  [TRUST_REGION_BO_MICRO]\n",
      "  Week 9 query: 0.790638-0.361606-0.715460-0.410503-0.238615-0.878998\n",
      "  GP kernel: 8.5**2 * Matern(length_scale=[100, 100, 100, 1.66, 0.361, 100], nu=2.5) + WhiteKernel(noise_level=1e-12)\n",
      "  local-train K: 6 | xi: 1e-08\n",
      "  local_sigma: 0.003 | radius: 0.0075\n",
      "  candidates kept: 41734 | best EI: 2.302894e-02\n",
      "  best observed y: 1.086263 at x_best=0.789513-0.359715-0.716440-0.416791-0.231308-0.873588\n",
      "\n",
      "F8  [RIDGE_MICRO]\n",
      "  Week 9 query: 0.103545-0.233545-0.353545-0.500000-0.620000-0.740000-0.860000-0.960000\n",
      "  tuned alpha: 0.0001 | LOOCV MSE: 0.016562 | tuned step: 0.0025\n",
      "  best observed y: 8.760408 at x_best=0.104988-0.234988-0.354988-0.500000-0.620000-0.740000-0.860000-0.960000\n",
      "\n",
      "Done. Paste each Week 9 query string into its function field.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "# ============================================================\n",
    "# WEEK 9 QUERY GENERATOR (Weeks 1â€“8 data)\n",
    "#\n",
    "# Your Week 9 strategy (as requested):\n",
    "#   âš« F1 : Ignore / minimal (cheap, low-effort point)\n",
    "#   ðŸŸ¡ F2 : Ridge regression, VERY small steps (near convergence)\n",
    "#   ðŸŸ¡ F3/F4/F8 : Micro-refinement only (Ridge + tiny steps)\n",
    "#   ðŸ”´ F5 : Trust-region BO (LOCAL ONLY), reduce local_sigma, micro-exploitation\n",
    "#   ðŸŸ¢ F7 : Same as F5 but even tighter (6D)\n",
    "#   ðŸ”µ F6 : Manual continue direction (small gamma)\n",
    "#\n",
    "# Objective assumed: MAXIMISE y for each function\n",
    "# (so \"less negative\" is better for F3/F4/F6)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# sklearn availability checks\n",
    "# ----------------------------\n",
    "try:\n",
    "    from sklearn.linear_model import Ridge\n",
    "    SKLEARN_RIDGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_RIDGE_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    SKLEARN_GP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_GP_AVAILABLE = False\n",
    "\n",
    "# GP convergence warnings are common with tiny datasets; not fatal.\n",
    "if SKLEARN_GP_AVAILABLE:\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) DATA: Weeks 1â€“8 (X inputs, y outputs) for F1..F8\n",
    "#    (Updated to include Week 8 results you shared)\n",
    "# ============================================================\n",
    "DATA = {\n",
    "    \"F1\": {\n",
    "        \"X\": np.array([\n",
    "            [0.145000, 0.515000],  # W1\n",
    "            [0.725000, 0.285000],  # W2\n",
    "            [0.515000, 0.515000],  # W3\n",
    "            [0.750000, 0.750000],  # W4\n",
    "            [0.990000, 0.010000],  # W5\n",
    "            [0.000029, 0.001417],  # W6\n",
    "            [0.305976, 0.997403],  # W7\n",
    "            [0.422868, 0.002773],  # W8\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            -3.353165630322361e-61,\n",
    "            6.743225602289377e-78,\n",
    "            4.714509345171323e-13,\n",
    "            1.3319145509281447e-22,\n",
    "            0.0,\n",
    "            1.825040909472812e-247,\n",
    "            -1.5662072753465034e-167,\n",
    "            -7.806084086345555e-123,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F2\": {\n",
    "        \"X\": np.array([\n",
    "            [0.755000, 0.275000],  # W1\n",
    "            [0.785000, 0.305000],  # W2\n",
    "            [0.740000, 0.260000],  # W3\n",
    "            [0.730000, 0.270000],  # W4\n",
    "            [0.718763, 0.261649],  # W5\n",
    "            [0.722018, 0.263976],  # W6\n",
    "            [0.721323, 0.261711],  # W7\n",
    "            [0.724285, 0.264402],  # W8\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            0.42044085041824825,\n",
    "            -0.0456643112924181,\n",
    "            0.46274019045813003,\n",
    "            0.6060955609811236,\n",
    "            0.5195146975906033,\n",
    "            0.5794253005452772,\n",
    "            0.5796694237276565,\n",
    "            0.6272586156230583,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F3\": {\n",
    "        \"X\": np.array([\n",
    "            [0.395000, 0.875000, 0.635000],  # W1\n",
    "            [0.145000, 0.395000, 0.915000],  # W2\n",
    "            [0.120000, 0.347000, 0.943000],  # W3\n",
    "            [0.155000, 0.385000, 0.905000],  # W4\n",
    "            [0.165000, 0.375000, 0.895000],  # W5\n",
    "            [0.178771, 0.372140, 0.880781],  # W6\n",
    "            [0.184441, 0.353663, 0.875638],  # W7\n",
    "            [0.181867, 0.354586, 0.878279],  # W8\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            -0.12080733985523133,\n",
    "            -0.11535196594300248,\n",
    "            -0.20076336857175398,\n",
    "            -0.07852077254038155,\n",
    "            -0.06033571734237718,\n",
    "            -0.04739292498526722,\n",
    "            -0.05056402944032541,\n",
    "            -0.0478844185459012,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F4\": {\n",
    "        \"X\": np.array([\n",
    "            [0.275000, 0.955000, 0.515000, 0.145000],  # W1\n",
    "            [0.815000, 0.245000, 0.355000, 0.695000],  # W2\n",
    "            [0.869000, 0.174000, 0.339000, 0.750000],  # W3\n",
    "            [0.795000, 0.265000, 0.365000, 0.665000],  # W4\n",
    "            [0.785000, 0.275000, 0.370000, 0.650000],  # W5 (best so far)\n",
    "            [0.792676, 0.264502, 0.367988, 0.657198],  # W6\n",
    "            [0.791656, 0.265832, 0.368297, 0.656143],  # W7\n",
    "            [0.789610, 0.268623, 0.368839, 0.654212],  # W8\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            -18.59723490448631,\n",
    "            -14.395540985679897,\n",
    "            -18.67377341401988,\n",
    "            -13.169944884454413,\n",
    "            -12.699964227491282,\n",
    "            -12.987699814058924,\n",
    "            -12.94099410856025,\n",
    "            -12.85705507882481,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F5\": {\n",
    "        \"X\": np.array([\n",
    "            [0.635000, 0.395000, 0.755000, 0.875000],  # W1\n",
    "            [0.665000, 0.365000, 0.785000, 0.845000],  # W2\n",
    "            [0.680000, 0.350000, 0.800000, 0.830000],  # W3\n",
    "            [0.695000, 0.335000, 0.815000, 0.815000],  # W4\n",
    "            [0.707000, 0.323000, 0.827000, 0.803000],  # W5\n",
    "            [0.728000, 0.302000, 0.848000, 0.782000],  # W6\n",
    "            [0.591139, 0.057257, 0.976087, 0.523586],  # W7 (bad global jump)\n",
    "            [0.745989, 0.305287, 0.849251, 0.788893],  # W8 (best)\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            287.4343816627659,\n",
    "            292.2593658119571,\n",
    "            301.5311905557768,\n",
    "            315.65049985154724,\n",
    "            330.66611638919255,\n",
    "            365.66328225833024,\n",
    "            283.75880106841055,\n",
    "            413.12789189378645,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F6\": {\n",
    "        \"X\": np.array([\n",
    "            [0.515000, 0.145000, 0.955000, 0.395000, 0.755000],  # W1\n",
    "            [0.185000, 0.745000, 0.315000, 0.865000, 0.455000],  # W2\n",
    "            [0.152000, 0.805000, 0.251000, 0.912000, 0.425000],  # W3\n",
    "            [0.170000, 0.760000, 0.300000, 0.890000, 0.470000],  # W4\n",
    "            [0.200000, 0.730000, 0.330000, 0.840000, 0.455000],  # W5\n",
    "            [0.218000, 0.712000, 0.348000, 0.810000, 0.446000],  # W6\n",
    "            [0.228800, 0.701200, 0.358800, 0.792000, 0.440600],  # W7\n",
    "            [0.235280, 0.694720, 0.365280, 0.781200, 0.437360],  # W8 (best)\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            -1.6304531811460896,\n",
    "            -1.4347679755670883,\n",
    "            -1.6451191179236977,\n",
    "            -1.6022183821509282,\n",
    "            -1.3295280103104827,\n",
    "            -1.2429202946292475,\n",
    "            -1.2012624047628697,\n",
    "            -1.1222958408999941,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F7\": {\n",
    "        \"X\": np.array([\n",
    "            [0.875000, 0.275000, 0.635000, 0.515000, 0.145000, 0.955000],  # W1\n",
    "            [0.845000, 0.305000, 0.665000, 0.485000, 0.175000, 0.925000],  # W2\n",
    "            [0.830000, 0.320000, 0.680000, 0.470000, 0.190000, 0.910000],  # W3\n",
    "            [0.815000, 0.335000, 0.695000, 0.455000, 0.205000, 0.895000],  # W4\n",
    "            [0.805202, 0.344798, 0.704798, 0.445202, 0.214798, 0.885202],  # W5\n",
    "            [0.791730, 0.358270, 0.718270, 0.431730, 0.228270, 0.871730],  # W6\n",
    "            [0.013373, 0.928169, 0.299072, 0.839656, 0.777563, 0.029987],  # W7 (bad global jump)\n",
    "            [0.789513, 0.359715, 0.716440, 0.416791, 0.231308, 0.873588],  # W8 (best)\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            0.6267064847700778,\n",
    "            0.8069621926499697,\n",
    "            0.8919314248129555,\n",
    "            0.969339703275594,\n",
    "            1.0144420450032012,\n",
    "            1.0679017392374972,\n",
    "            0.10868500160826922,\n",
    "            1.0862632473367084,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F8\": {\n",
    "        \"X\": np.array([\n",
    "            [0.145000, 0.275000, 0.395000, 0.515000, 0.635000, 0.755000, 0.875000, 0.955000],  # W1\n",
    "            [0.175000, 0.305000, 0.425000, 0.545000, 0.665000, 0.785000, 0.905000, 0.945000],  # W2\n",
    "            [0.130000, 0.260000, 0.380000, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W3\n",
    "            [0.140000, 0.270000, 0.390000, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W4\n",
    "            [0.120000, 0.250000, 0.370000, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W5\n",
    "            [0.114226, 0.244226, 0.364226, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W6\n",
    "            [0.109607, 0.239607, 0.359607, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W7\n",
    "            [0.104988, 0.234988, 0.354988, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W8 (best)\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            8.633935,\n",
    "            8.451335,\n",
    "            8.71814,\n",
    "            8.69914,\n",
    "            8.73594,\n",
    "            8.745671245544,\n",
    "            8.753167873306,\n",
    "            8.760408479136,\n",
    "        ], float),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Formatting + utility helpers\n",
    "# ============================================================\n",
    "def clip_01(x):\n",
    "    \"\"\"Clip to [0, 0.999999] so portal always shows values starting with '0.'\"\"\"\n",
    "    return np.clip(np.asarray(x, float), 0.0, 0.999999)\n",
    "\n",
    "def format_query(x):\n",
    "    \"\"\"Portal query string: 6 decimals, hyphen-separated.\"\"\"\n",
    "    return \"-\".join(f\"{v:.6f}\" for v in np.asarray(x, float))\n",
    "\n",
    "def ensure_unique_after_rounding(x, X_existing, seed=0, tries=200, jitter=5e-6):\n",
    "    \"\"\"\n",
    "    Portal rounds to 6 decimals; two different floats can become identical strings.\n",
    "    This nudges x slightly if needed to avoid an accidental duplicate submission.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    existing_str = {format_query(row) for row in np.asarray(X_existing, float)}\n",
    "    x = np.asarray(x, float).copy()\n",
    "\n",
    "    for _ in range(tries):\n",
    "        if format_query(x) not in existing_str:\n",
    "            return clip_01(x)\n",
    "        x = x + rng.normal(0.0, jitter, size=x.shape)\n",
    "        x = clip_01(x)\n",
    "\n",
    "    return clip_01(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) F1 (ignore/minimal): cheap space-filling point\n",
    "# ============================================================\n",
    "def propose_F1_minimal(X_existing, n_rand=3000, seed=9):\n",
    "    \"\"\"\n",
    "    Minimal effort: pick a point that is far from previous points (maximin)\n",
    "    using a small random candidate set.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = X_existing.shape[1]\n",
    "\n",
    "    R = rng.random((n_rand, d)) * 0.999999\n",
    "    # distance to closest existing point\n",
    "    dists = np.sqrt(((R[:, None, :] - X_existing[None, :, :]) ** 2).sum(axis=2))\n",
    "    min_d = dists.min(axis=1)\n",
    "\n",
    "    x = R[int(np.argmax(min_d))]\n",
    "    return ensure_unique_after_rounding(x, X_existing, seed=seed + 100, jitter=1e-5)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) F6 (manual): continue last improving direction (damped)\n",
    "# ============================================================\n",
    "def propose_F6_continue_direction(X, y, gamma=0.55):\n",
    "    \"\"\"\n",
    "    Simple heuristic:\n",
    "      - Look at last move delta = x_last - x_prev\n",
    "      - If y improved, keep moving in same direction (scaled by gamma)\n",
    "      - Else reverse direction\n",
    "    gamma < 1 makes it a smaller step (micro refinement).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "\n",
    "    x_prev, x_last = X[-2].copy(), X[-1].copy()\n",
    "    y_prev, y_last = float(y[-2]), float(y[-1])\n",
    "\n",
    "    delta = x_last - x_prev\n",
    "    x_new = x_last + gamma * delta if y_last >= y_prev else x_last - gamma * delta\n",
    "    x_new = clip_01(x_new)\n",
    "\n",
    "    return ensure_unique_after_rounding(x_new, X, seed=606, jitter=3e-6)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Ridge (F2,F3,F4,F8): LOOCV alpha + MICRO step tuning\n",
    "#    Week 9: we shrink step sizes aggressively to do micro refinement.\n",
    "# ============================================================\n",
    "ALPHAS = [1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n",
    "\n",
    "# Micro step grids (smaller than Week 8)\n",
    "STEP_GRID = {\n",
    "    \"F2\": [0.0008, 0.0012, 0.0016, 0.0020],     # almost converged\n",
    "    \"F3\": [0.0020, 0.0035, 0.0050, 0.0065],     # micro refinement\n",
    "    \"F4\": [0.0015, 0.0025, 0.0035, 0.0045],     # micro refinement\n",
    "    \"F8\": [0.0010, 0.0015, 0.0020, 0.0025],     # micro refinement\n",
    "}\n",
    "\n",
    "# F8: keep template stable; only adjust first 3 dims (this matches your pattern)\n",
    "MASK = {\"F8\": [0, 1, 2]}\n",
    "\n",
    "def ridge_fit(X, y, alpha):\n",
    "    \"\"\"\n",
    "    Fit ridge regression: y â‰ˆ b0 + X b\n",
    "    Using sklearn Ridge if available.\n",
    "    \"\"\"\n",
    "    if not SKLEARN_RIDGE_AVAILABLE:\n",
    "        raise ImportError(\"scikit-learn Ridge not available. Install scikit-learn or add a closed-form fallback.\")\n",
    "\n",
    "    model = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    model.fit(X, y)\n",
    "    return float(model.intercept_), model.coef_.astype(float)\n",
    "\n",
    "def ridge_predict(b0, b, X):\n",
    "    return b0 + np.asarray(X, float) @ np.asarray(b, float)\n",
    "\n",
    "def choose_alpha_loocv(X, y, alphas):\n",
    "    \"\"\"\n",
    "    Leave-One-Out CV to choose alpha.\n",
    "    We do this on z-scored y for numerical stability.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    best_a, best_mse = None, np.inf\n",
    "    for a in alphas:\n",
    "        errs = []\n",
    "        for i in range(n):\n",
    "            m = np.ones(n, dtype=bool)\n",
    "            m[i] = False\n",
    "            b0, b = ridge_fit(X[m], y[m], alpha=a)\n",
    "            yhat = ridge_predict(b0, b, X[~m])[0]\n",
    "            errs.append((y[~m][0] - yhat) ** 2)\n",
    "\n",
    "        mse = float(np.mean(errs))\n",
    "        if mse < best_mse:\n",
    "            best_mse, best_a = mse, a\n",
    "\n",
    "    return best_a, best_mse\n",
    "\n",
    "def apply_mask(v, mask):\n",
    "    v = np.asarray(v, float)\n",
    "    if mask is None:\n",
    "        return v\n",
    "    out = np.zeros_like(v)\n",
    "    out[mask] = v[mask]\n",
    "    return out\n",
    "\n",
    "def ridge_candidates(x_best, b, step, mask=None):\n",
    "    \"\"\"\n",
    "    Candidate generation around x_best (micro refinement):\n",
    "      - small move along +b direction (scaled)\n",
    "      - coordinate nudges based on sign(b_j)\n",
    "    \"\"\"\n",
    "    x_best = np.asarray(x_best, float)\n",
    "    b_eff = apply_mask(b, mask)\n",
    "\n",
    "    norm = np.linalg.norm(b_eff)\n",
    "    cands = []\n",
    "\n",
    "    if norm < 1e-12:\n",
    "        # flat model: coordinate perturbations only\n",
    "        for j in range(x_best.size):\n",
    "            if mask is not None and j not in mask:\n",
    "                continue\n",
    "            for s in [step, 0.5 * step]:\n",
    "                x1 = x_best.copy(); x1[j] += s; cands.append(x1)\n",
    "                x2 = x_best.copy(); x2[j] -= s; cands.append(x2)\n",
    "        return [clip_01(c) for c in cands]\n",
    "\n",
    "    direction = b_eff / norm\n",
    "\n",
    "    # along direction (micro)\n",
    "    for m in [0.5, 1.0]:\n",
    "        cands.append(x_best + (m * step) * direction)\n",
    "\n",
    "    # coordinate-wise safe nudges\n",
    "    for j in range(x_best.size):\n",
    "        if mask is not None and j not in mask:\n",
    "            continue\n",
    "        sgn = np.sign(b_eff[j])\n",
    "        if sgn == 0:\n",
    "            continue\n",
    "        xj = x_best.copy()\n",
    "        xj[j] += 0.8 * step * sgn\n",
    "        cands.append(xj)\n",
    "\n",
    "    return [clip_01(c) for c in cands]\n",
    "\n",
    "def propose_ridge_next(fname, X, y):\n",
    "    \"\"\"\n",
    "    Ridge proposal rule:\n",
    "      1) choose x_best = argmax y (best observed)\n",
    "      2) z-score y (stability)\n",
    "      3) choose alpha with LOOCV\n",
    "      4) fit ridge on all data\n",
    "      5) generate micro candidates around x_best and choose max predicted\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "\n",
    "    best_idx = int(np.argmax(y))\n",
    "    x_best = X[best_idx].copy()\n",
    "    y_best = float(y[best_idx])\n",
    "\n",
    "    # z-score y\n",
    "    y_mean, y_std = float(np.mean(y)), float(np.std(y))\n",
    "    if y_std < 1e-12:\n",
    "        y_std = 1.0\n",
    "    y_z = (y - y_mean) / y_std\n",
    "\n",
    "    alpha, loocv_mse = choose_alpha_loocv(X, y_z, ALPHAS)\n",
    "    b0, b = ridge_fit(X, y_z, alpha=alpha)\n",
    "\n",
    "    mask = MASK.get(fname, None)\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_step = None\n",
    "    best_x = None\n",
    "\n",
    "    for step in STEP_GRID[fname]:\n",
    "        cands = ridge_candidates(x_best, b, step, mask=mask)\n",
    "        preds = ridge_predict(b0, b, np.array(cands))\n",
    "        idx = int(np.argmax(preds))\n",
    "        if float(preds[idx]) > best_score:\n",
    "            best_score = float(preds[idx])\n",
    "            best_step = step\n",
    "            best_x = cands[idx]\n",
    "\n",
    "    best_x = ensure_unique_after_rounding(best_x, X, seed=hash(fname) % 10000, jitter=2e-6)\n",
    "\n",
    "    return {\n",
    "        \"x_next\": best_x,\n",
    "        \"alpha\": alpha,\n",
    "        \"loocv_mse\": loocv_mse,\n",
    "        \"step\": best_step,\n",
    "        \"x_best\": x_best,\n",
    "        \"y_best\": y_best,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Trust-region BO micro exploitation (F5,F7)\n",
    "#    - LOCAL candidates only (no global exploration)\n",
    "#    - Very small local_sigma\n",
    "#    - Fit GP on LOCAL subset (nearest K points to x_best) to ignore outliers\n",
    "# ============================================================\n",
    "def stdnorm_pdf(z):\n",
    "    return np.exp(-0.5 * z * z) / math.sqrt(2.0 * math.pi)\n",
    "\n",
    "def stdnorm_cdf(z):\n",
    "    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "\n",
    "def expected_improvement(mu, sigma, y_best, xi=1e-8):\n",
    "    \"\"\"\n",
    "    Expected Improvement for maximisation.\n",
    "    Very small xi -> micro-exploitation (EI ~ choose high mean with a bit of uncertainty bonus).\n",
    "    \"\"\"\n",
    "    mu = np.asarray(mu, float)\n",
    "    sigma = np.asarray(sigma, float)\n",
    "\n",
    "    ei = np.zeros_like(mu)\n",
    "    m = sigma > 1e-12\n",
    "    imp = mu[m] - y_best - xi\n",
    "    Z = imp / sigma[m]\n",
    "\n",
    "    # Using python math.erf; list comprehension is okay at these candidate sizes.\n",
    "    Phi = np.array([stdnorm_cdf(z) for z in Z])\n",
    "    phi = np.array([stdnorm_pdf(z) for z in Z])\n",
    "\n",
    "    ei[m] = imp * Phi + sigma[m] * phi\n",
    "    ei[ei < 0] = 0.0\n",
    "    return ei\n",
    "\n",
    "def fit_gp(X, y, seed=0):\n",
    "    \"\"\"\n",
    "    GP fit (local surrogate).\n",
    "    We add:\n",
    "      - WhiteKernel with very low lower bound (avoid \"noise at lower bound\" warnings)\n",
    "      - alpha nugget for numerical stability\n",
    "    \"\"\"\n",
    "    if not SKLEARN_GP_AVAILABLE:\n",
    "        raise ImportError(\"GaussianProcessRegressor not available. Install scikit-learn to use BO.\")\n",
    "\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "    d = X.shape[1]\n",
    "\n",
    "    kernel = (\n",
    "        C(1.0, (1e-3, 1e3)) *\n",
    "        Matern(length_scale=np.ones(d), length_scale_bounds=(1e-2, 1e2), nu=2.5) +\n",
    "        WhiteKernel(noise_level=1e-6, noise_level_bounds=(1e-12, 1e-2))\n",
    "    )\n",
    "\n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        normalize_y=True,\n",
    "        n_restarts_optimizer=4,\n",
    "        random_state=seed,\n",
    "        alpha=1e-10,  # small jitter for stability\n",
    "    )\n",
    "    gp.fit(X, y)\n",
    "    return gp\n",
    "\n",
    "def propose_bo_trust_region_micro(\n",
    "    fname, X, y,\n",
    "    seed=0,\n",
    "    xi=1e-8,\n",
    "    n_local=35000,\n",
    "    local_sigma=0.0045,\n",
    "    radius_mult=2.5,\n",
    "    k_local_train=6\n",
    "):\n",
    "    \"\"\"\n",
    "    Trust-region BO (micro exploitation):\n",
    "      1) Identify x_best (best observed)\n",
    "      2) Fit GP on k nearest historical points to x_best (local training set)\n",
    "      3) Sample ONLY local candidates around x_best with very small sigma\n",
    "      4) Enforce hard trust-region per coordinate (|dx_i| <= radius_mult*sigma)\n",
    "      5) Pick argmax EI\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    best_idx = int(np.argmax(y))\n",
    "    x_best = X[best_idx].copy()\n",
    "    y_best = float(y[best_idx])\n",
    "\n",
    "    # ---- Local training subset (nearest K points to x_best) ----\n",
    "    dists = np.sqrt(((X - x_best) ** 2).sum(axis=1))\n",
    "    k = min(int(k_local_train), X.shape[0])\n",
    "    idxs = np.argsort(dists)[:k]\n",
    "    X_train = X[idxs]\n",
    "    y_train = y[idxs]\n",
    "\n",
    "    gp = fit_gp(X_train, y_train, seed=seed)\n",
    "\n",
    "    d = X.shape[1]\n",
    "    radius = radius_mult * local_sigma\n",
    "\n",
    "    # ---- Local candidates only ----\n",
    "    CANDS = x_best + rng.normal(0.0, local_sigma, size=(n_local, d))\n",
    "    CANDS = clip_01(CANDS)\n",
    "\n",
    "    # Hard trust-region filter (prevents tail events causing jumps)\n",
    "    diff = np.abs(CANDS - x_best[None, :])\n",
    "    CANDS = CANDS[(diff <= radius).all(axis=1)]\n",
    "\n",
    "    if CANDS.shape[0] == 0:\n",
    "        # fallback (rare)\n",
    "        CANDS = clip_01(x_best + rng.normal(0.0, local_sigma, size=(n_local, d)))\n",
    "\n",
    "    # Remove points extremely close to existing points (float-space)\n",
    "    dist2 = ((CANDS[:, None, :] - X[None, :, :]) ** 2).sum(axis=2)\n",
    "    min_d = np.sqrt(dist2.min(axis=1))\n",
    "    CANDS = CANDS[min_d > 1e-6]\n",
    "\n",
    "    if CANDS.shape[0] == 0:\n",
    "        CANDS = clip_01(x_best + rng.normal(0.0, local_sigma, size=(n_local, d)))\n",
    "\n",
    "    mu, sigma = gp.predict(CANDS, return_std=True)\n",
    "    ei = expected_improvement(mu, sigma, y_best=y_best, xi=xi)\n",
    "\n",
    "    idx = int(np.argmax(ei))\n",
    "    x_next = ensure_unique_after_rounding(CANDS[idx], X, seed=seed + 999, jitter=2e-6)\n",
    "\n",
    "    return {\n",
    "        \"x_next\": x_next,\n",
    "        \"x_best\": x_best,\n",
    "        \"y_best\": y_best,\n",
    "        \"kernel_\": str(gp.kernel_),\n",
    "        \"xi\": xi,\n",
    "        \"local_sigma\": local_sigma,\n",
    "        \"radius\": radius,\n",
    "        \"best_ei\": float(ei[idx]),\n",
    "        \"n_candidates\": int(CANDS.shape[0]),\n",
    "        \"k_local_train\": k,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) BUILD WEEK 9 PLAN (per your mapping)\n",
    "# ============================================================\n",
    "PLAN = {}\n",
    "\n",
    "# âš« F1 minimal / ignore\n",
    "PLAN[\"F1\"] = (\"MINIMAL_IGNORE\", {\"x_next\": propose_F1_minimal(DATA[\"F1\"][\"X\"], n_rand=3000, seed=9)})\n",
    "\n",
    "# ðŸŸ¡ Ridge micro: F2, F3, F4, F8\n",
    "for f in [\"F2\", \"F3\", \"F4\", \"F8\"]:\n",
    "    PLAN[f] = (\"RIDGE_MICRO\", propose_ridge_next(f, DATA[f][\"X\"], DATA[f][\"y\"]))\n",
    "\n",
    "# ðŸ”´ F5 trust-region BO micro (slightly reduced sigma vs Week 8)\n",
    "PLAN[\"F5\"] = (\"TRUST_REGION_BO_MICRO\", propose_bo_trust_region_micro(\n",
    "    \"F5\", DATA[\"F5\"][\"X\"], DATA[\"F5\"][\"y\"],\n",
    "    seed=205,\n",
    "    xi=1e-8,            # micro-exploitation\n",
    "    n_local=35000,\n",
    "    local_sigma=0.0045, # reduced vs Week 8 (0.006)\n",
    "    radius_mult=2.5,\n",
    "    k_local_train=6\n",
    "))\n",
    "\n",
    "# ðŸŸ¢ F7 trust-region BO micro (even tighter than F5)\n",
    "PLAN[\"F7\"] = (\"TRUST_REGION_BO_MICRO\", propose_bo_trust_region_micro(\n",
    "    \"F7\", DATA[\"F7\"][\"X\"], DATA[\"F7\"][\"y\"],\n",
    "    seed=207,\n",
    "    xi=1e-8,            # micro-exploitation\n",
    "    n_local=45000,      # more candidates because 6D\n",
    "    local_sigma=0.0030, # tighter than Week 8 (0.005)\n",
    "    radius_mult=2.5,\n",
    "    k_local_train=6\n",
    "))\n",
    "\n",
    "# ðŸ”µ F6 manual micro-continue\n",
    "PLAN[\"F6\"] = (\"MANUAL_MICRO\", {\"x_next\": propose_F6_continue_direction(\n",
    "    DATA[\"F6\"][\"X\"], DATA[\"F6\"][\"y\"], gamma=0.55\n",
    ")})\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) PRINT WEEK 9 QUERIES FOR PORTAL (F1..F8)\n",
    "# ============================================================\n",
    "print(\"==== WEEK 9 QUERIES (COPY/PASTE INTO PORTAL) ====\\n\")\n",
    "\n",
    "for f in [\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F7\", \"F8\"]:\n",
    "    method, info = PLAN[f]\n",
    "    x_next = info[\"x_next\"]\n",
    "\n",
    "    print(f\"{f}  [{method}]\")\n",
    "    print(f\"  Week 9 query: {format_query(x_next)}\")\n",
    "\n",
    "    # Diagnostics for your Week 9 reflection\n",
    "    if method == \"RIDGE_MICRO\":\n",
    "        print(f\"  tuned alpha: {info['alpha']} | LOOCV MSE: {info['loocv_mse']:.6f} | tuned step: {info['step']}\")\n",
    "        print(f\"  best observed y: {info['y_best']:.6f} at x_best={format_query(info['x_best'])}\")\n",
    "\n",
    "    if method == \"TRUST_REGION_BO_MICRO\":\n",
    "        print(f\"  GP kernel: {info['kernel_']}\")\n",
    "        print(f\"  local-train K: {info['k_local_train']} | xi: {info['xi']}\")\n",
    "        print(f\"  local_sigma: {info['local_sigma']} | radius: {info['radius']}\")\n",
    "        print(f\"  candidates kept: {info['n_candidates']} | best EI: {info['best_ei']:.6e}\")\n",
    "        print(f\"  best observed y: {info['y_best']:.6f} at x_best={format_query(info['x_best'])}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print(\"Done. Paste each Week 9 query string into its function field.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faee50f-4497-4464-8a5e-31383d7f019c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
