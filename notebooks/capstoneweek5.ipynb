{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d8a8eb9-0c3d-4891-ac53-15767e3b2fe8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sklearn: True\n",
      "Ridge alpha = 0.01\n",
      "\n",
      "F2\n",
      "  Best-so-far x : 0.730000-0.270000   y: 0.606096\n",
      "  Ridge intercept b0: 1.862963\n",
      "  Ridge coeffs b (≈ local gradient direction): [-1.566729 -1.164311]\n",
      "  Unit direction b/||b||: [-0.802632 -0.596474]\n",
      "  Step size: 0.014\n",
      "  Week 5 x (suggested): 0.718763-0.261649\n",
      "\n",
      "F5\n",
      "  Best-so-far x : 0.695000-0.335000-0.815000-0.815000   y: 315.650500\n",
      "  Ridge intercept b0: 286.648344\n",
      "  Ridge coeffs b (≈ local gradient direction): [ 49.296141 -49.296141  49.296141 -49.296141]\n",
      "  Unit direction b/||b||: [ 0.5 -0.5  0.5 -0.5]\n",
      "  Step size: 0.024\n",
      "  Week 5 x (suggested): 0.707000-0.323000-0.827000-0.803000\n",
      "\n",
      "F7\n",
      "  Best-so-far x : 0.815000-0.335000-0.695000-0.455000-0.205000-0.895000   y: 0.969340\n",
      "  Ridge intercept b0: 1.387213\n",
      "  Ridge coeffs b (≈ local gradient direction): [-0.518141  0.518141  0.518141 -0.518141  0.518141 -0.518141]\n",
      "  Unit direction b/||b||: [-0.408248  0.408248  0.408248 -0.408248  0.408248 -0.408248]\n",
      "  Step size: 0.024\n",
      "  Week 5 x (suggested): 0.805202-0.344798-0.704798-0.445202-0.214798-0.885202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Ridge regression to propose Week 5 queries (F2, F5, F7)\n",
    "# ============================================================\n",
    "# CONTEXT (BBO challenge)\n",
    "# ----------------------\n",
    "# You have a black-box function f(x) for each objective (F2/F5/F7).\n",
    "# Each week you submit ONE x (a vector in [0,1]^d), and you get ONE output y=f(x).\n",
    "# By Week 4 you have 4 samples per function: (x^(1), y^(1)), ..., (x^(4), y^(4)).\n",
    "#\n",
    "# Goal for Week 5:\n",
    "#   Choose a new x^(5) that (hopefully) increases y.\n",
    "#\n",
    "# Idea used in this notebook:\n",
    "#   1) Fit a *simple surrogate model* y_hat(x) ≈ b0 + b^T x using Ridge regression\n",
    "#   2) Use its coefficients b as an estimate of the *local uphill direction*\n",
    "#   3) Take a small step from the best-so-far point in that direction\n",
    "#\n",
    "# Why Ridge?\n",
    "#   With only 4 data points, a plain linear regression can overfit and give unstable coefficients.\n",
    "#   Ridge adds L2 regularisation to stabilise the coefficient estimates.\n",
    "#\n",
    "# Ridge objective (fit a linear model):\n",
    "#   minimize_{b0,b}  Σ (y_i - (b0 + b^T x_i))^2  +  α ||b||^2\n",
    "# where:\n",
    "#   - b0 is intercept (not regularised)\n",
    "#   - b are coefficients (regularised)\n",
    "#   - α controls regularisation strength:\n",
    "#       α small  -> more flexible (risk overfit)\n",
    "#       α large  -> more stable but may underfit\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Try scikit-learn Ridge; fallback to a closed-form ridge if sklearn isn't available.\n",
    "# In Jupyter, you likely have sklearn installed, but this makes the notebook portable.\n",
    "try:\n",
    "    from sklearn.linear_model import Ridge\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Your Weeks 1–4 data (inputs X, outputs y)\n",
    "# ----------------------------\n",
    "# Each function stores:\n",
    "#   X: shape (n_samples=4, d)\n",
    "#   y: shape (n_samples=4,)\n",
    "#   step: the step size used to move from best x in the ridge \"uphill\" direction.\n",
    "#\n",
    "# Step sizes:\n",
    "#   - F2 is sensitive -> smaller step\n",
    "#   - F5 is smooth and strongly improving -> moderate step\n",
    "#   - F7 is high-dimensional -> moderate but cautious step\n",
    "DATA = {\n",
    "    \"F2\": {\n",
    "        \"X\": np.array([\n",
    "            [0.755000, 0.275000],  # Week 1\n",
    "            [0.785000, 0.305000],  # Week 2\n",
    "            [0.740000, 0.260000],  # Week 3\n",
    "            [0.730000, 0.270000],  # Week 4\n",
    "        ], dtype=float),\n",
    "        \"y\": np.array([\n",
    "            0.42044085041824825,\n",
    "            -0.0456643112924181,\n",
    "            0.46274019045813003,\n",
    "            0.6060955609811236,\n",
    "        ], dtype=float),\n",
    "        \"step\": 0.014,\n",
    "    },\n",
    "\n",
    "    \"F5\": {\n",
    "        \"X\": np.array([\n",
    "            [0.635000, 0.395000, 0.755000, 0.875000],  # Week 1\n",
    "            [0.665000, 0.365000, 0.785000, 0.845000],  # Week 2\n",
    "            [0.680000, 0.350000, 0.800000, 0.830000],  # Week 3\n",
    "            [0.695000, 0.335000, 0.815000, 0.815000],  # Week 4\n",
    "        ], dtype=float),\n",
    "        \"y\": np.array([\n",
    "            287.4343816627659,\n",
    "            292.2593658119571,\n",
    "            301.5311905557768,\n",
    "            315.65049985154724,\n",
    "        ], dtype=float),\n",
    "        \"step\": 0.024,\n",
    "    },\n",
    "\n",
    "    \"F7\": {\n",
    "        \"X\": np.array([\n",
    "            [0.875000, 0.275000, 0.635000, 0.515000, 0.145000, 0.955000],  # Week 1\n",
    "            [0.845000, 0.305000, 0.665000, 0.485000, 0.175000, 0.925000],  # Week 2\n",
    "            [0.830000, 0.320000, 0.680000, 0.470000, 0.190000, 0.910000],  # Week 3\n",
    "            [0.815000, 0.335000, 0.695000, 0.455000, 0.205000, 0.895000],  # Week 4\n",
    "        ], dtype=float),\n",
    "        \"y\": np.array([\n",
    "            0.6267064847700778,\n",
    "            0.8069621926499697,\n",
    "            0.8919314248129555,\n",
    "            0.969339703275594,\n",
    "        ], dtype=float),\n",
    "        \"step\": 0.024,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helper functions\n",
    "# ----------------------------\n",
    "def format_query(x: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Portal format: '0.xxxxxx-0.xxxxxx-...'\n",
    "    \"\"\"\n",
    "    return \"-\".join(f\"{v:.6f}\" for v in x)\n",
    "\n",
    "\n",
    "def clip_01(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Keep values within [0, 0.999999] so they always start with '0.' and remain valid.\n",
    "    (The portal expects values starting with 0 and 6 decimals.)\n",
    "    \"\"\"\n",
    "    return np.clip(x, 0.0, 0.999999)\n",
    "\n",
    "\n",
    "def fit_ridge_sklearn(X: np.ndarray, y: np.ndarray, alpha: float):\n",
    "    \"\"\"\n",
    "    Fit Ridge regression using scikit-learn.\n",
    "\n",
    "    The model is:\n",
    "        y_hat(x) = b0 + b^T x\n",
    "\n",
    "    scikit-learn solves:\n",
    "        min Σ (y_i - y_hat(x_i))^2 + alpha * ||b||^2\n",
    "\n",
    "    Returns:\n",
    "        b0: intercept\n",
    "        b : coefficient vector\n",
    "    \"\"\"\n",
    "    model = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    model.fit(X, y)\n",
    "    return float(model.intercept_), model.coef_.astype(float)\n",
    "\n",
    "\n",
    "def fit_ridge_closed_form(X: np.ndarray, y: np.ndarray, alpha: float):\n",
    "    \"\"\"\n",
    "    Closed-form Ridge regression (no sklearn required).\n",
    "\n",
    "    We rewrite the linear model with an intercept using an augmented matrix:\n",
    "        Z = [1, X]   (prepend a column of ones)\n",
    "\n",
    "    Then solve:\n",
    "        beta = (Z^T Z + alpha * I)^(-1) Z^T y\n",
    "\n",
    "    Important detail:\n",
    "        We do NOT regularise the intercept.\n",
    "        So we set I[0,0] = 0 (no penalty on intercept term).\n",
    "\n",
    "    Returns:\n",
    "        b0: intercept\n",
    "        b : coefficient vector\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    Z = np.hstack([np.ones((n, 1)), X])  # add intercept column\n",
    "    I = np.eye(d + 1)\n",
    "    I[0, 0] = 0.0  # do NOT regularise intercept\n",
    "\n",
    "    beta = np.linalg.solve(Z.T @ Z + alpha * I, Z.T @ y)\n",
    "    b0 = float(beta[0])\n",
    "    b = beta[1:].astype(float)\n",
    "    return b0, b\n",
    "\n",
    "\n",
    "def propose_week5(X: np.ndarray, y: np.ndarray, alpha: float, step: float):\n",
    "    \"\"\"\n",
    "    Core algorithm to propose Week 5 query:\n",
    "\n",
    "    Step 1) Fit Ridge regression surrogate:\n",
    "        y_hat(x) = b0 + b^T x\n",
    "\n",
    "    Step 2) Pick the best observed point so far:\n",
    "        x_best = argmax_y observed (x_i)\n",
    "\n",
    "    Step 3) Move a small step in the direction that increases y_hat:\n",
    "        For a linear model, gradient wrt x is:\n",
    "            ∇_x y_hat = b\n",
    "        So an \"uphill\" move is along +b.\n",
    "\n",
    "        We normalise b so 'step' has consistent meaning:\n",
    "            direction = b / ||b||\n",
    "            x_new = x_best + step * direction\n",
    "\n",
    "    Why normalise?\n",
    "        b's magnitude depends on scaling and regularisation.\n",
    "        Normalising makes step size comparable across functions.\n",
    "\n",
    "    Safety:\n",
    "        If ||b|| is ~0, the model is essentially flat -> no reliable direction.\n",
    "        Then we keep x_new = x_best.\n",
    "\n",
    "    Returns:\n",
    "        x_best, y_best, b0, b, x_new\n",
    "    \"\"\"\n",
    "    if SKLEARN_AVAILABLE:\n",
    "        b0, b = fit_ridge_sklearn(X, y, alpha=alpha)\n",
    "    else:\n",
    "        b0, b = fit_ridge_closed_form(X, y, alpha=alpha)\n",
    "\n",
    "    best_idx = int(np.argmax(y))\n",
    "    x_best = X[best_idx].copy()\n",
    "    y_best = float(y[best_idx])\n",
    "\n",
    "    norm = np.linalg.norm(b)\n",
    "    if norm < 1e-12:\n",
    "        # Model says: changing x doesn't affect y (or data too small -> unstable b)\n",
    "        x_new = x_best\n",
    "        direction = np.zeros_like(b)\n",
    "    else:\n",
    "        direction = b / norm\n",
    "        x_new = x_best + step * direction\n",
    "\n",
    "    x_new = clip_01(x_new)\n",
    "    return x_best, y_best, b0, b, direction, x_new\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run ridge + propose Week 5\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Regularisation strength:\n",
    "    # - Start with 1e-2 as a reasonable default for tiny datasets\n",
    "    # - You can try 1e-4, 1e-3, 1e-2, 1e-1 to see stability/sensitivity\n",
    "    alpha = 1e-2\n",
    "\n",
    "    print(f\"Using sklearn: {SKLEARN_AVAILABLE}\")\n",
    "    print(f\"Ridge alpha = {alpha}\\n\")\n",
    "\n",
    "    for fname, d in DATA.items():\n",
    "        X, y, step = d[\"X\"], d[\"y\"], d[\"step\"]\n",
    "\n",
    "        x_best, y_best, b0, b, direction, x_new = propose_week5(X, y, alpha=alpha, step=step)\n",
    "\n",
    "        print(f\"{fname}\")\n",
    "        print(f\"  Best-so-far x : {format_query(x_best)}   y: {y_best:.6f}\")\n",
    "        print(f\"  Ridge intercept b0: {b0:.6f}\")\n",
    "        print(f\"  Ridge coeffs b (≈ local gradient direction): {np.array2string(b, precision=6, suppress_small=False)}\")\n",
    "        print(f\"  Unit direction b/||b||: {np.array2string(direction, precision=6, suppress_small=False)}\")\n",
    "        print(f\"  Step size: {step}\")\n",
    "        print(f\"  Week 5 x (suggested): {format_query(x_new)}\")\n",
    "        print()\n",
    "\n",
    "# In a Jupyter notebook, you can just run main() in a cell.\n",
    "# If you copy into a .py script, the __name__ guard runs it automatically.\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d2db5-79bb-4a3a-ab97-3a046bbab4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
