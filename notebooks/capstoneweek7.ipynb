{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb634e04-02d0-4808-9881-a6255b87eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nish/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== WEEK 7 QUERIES (COPY/PASTE INTO PORTAL) ====\n",
      "\n",
      "F1  [EXPLORATION_MAXIMIN]\n",
      "  Week 7 query: 0.305976-0.997403\n",
      "\n",
      "F2  [BASIC_ML_RIDGE_TUNED]\n",
      "  Week 7 query: 0.721323-0.261711\n",
      "  tuned alpha: 0.0001 | LOOCV MSE: 0.879291 | tuned step: 0.012\n",
      "  best observed y: 0.606096 at x_best=0.730000-0.270000\n",
      "\n",
      "F3  [BASIC_ML_RIDGE_TUNED]\n",
      "  Week 7 query: 0.184441-0.353663-0.875638\n",
      "  tuned alpha: 1.0 | LOOCV MSE: 1.498796 | tuned step: 0.02\n",
      "  best observed y: -0.047393 at x_best=0.178771-0.372140-0.880781\n",
      "\n",
      "F4  [BASIC_ML_RIDGE_TUNED]\n",
      "  Week 7 query: 0.791656-0.265832-0.368297-0.656143\n",
      "  tuned alpha: 1.0 | LOOCV MSE: 1.697464 | tuned step: 0.013\n",
      "  best observed y: -12.699964 at x_best=0.785000-0.275000-0.370000-0.650000\n",
      "\n",
      "F5  [ADV_ML_BO_GP_EI_TR]\n",
      "  Week 7 query: 0.591139-0.057257-0.976087-0.523586\n",
      "  GP kernel: 11.1**2 * Matern(length_scale=[1.99, 0.917, 0.508, 0.465], nu=2.5) + WhiteKernel(noise_level=1e-06)\n",
      "  EI xi: 1e-06 | local_sigma: 0.02 | candidates: 32000 | best EI: 1.033150e+01\n",
      "  best observed y: 365.663282 at x_best=0.728000-0.302000-0.848000-0.782000\n",
      "\n",
      "F6  [MANUAL_CONTINUE_DIR]\n",
      "  Week 7 query: 0.228800-0.701200-0.358800-0.792000-0.440600\n",
      "\n",
      "F7  [ADV_ML_BO_GP_EI_TR]\n",
      "  Week 7 query: 0.031373-0.928169-0.299072-0.839656-0.777563-0.029987\n",
      "  GP kernel: 6.76**2 * Matern(length_scale=[0.995, 0.995, 0.995, 0.995, 0.995, 0.995], nu=2.5) + WhiteKernel(noise_level=1e-06)\n",
      "  EI xi: 1e-06 | local_sigma: 0.02 | candidates: 32000 | best EI: 2.222160e+00\n",
      "  best observed y: 1.067902 at x_best=0.791730-0.358270-0.718270-0.431730-0.228270-0.871730\n",
      "\n",
      "F8  [BASIC_ML_RIDGE_TUNED]\n",
      "  Week 7 query: 0.109607-0.239607-0.359607-0.500000-0.620000-0.740000-0.860000-0.960000\n",
      "  tuned alpha: 0.0001 | LOOCV MSE: 0.019555 | tuned step: 0.008\n",
      "  best observed y: 8.745671 at x_best=0.114226-0.244226-0.364226-0.500000-0.620000-0.740000-0.860000-0.960000\n",
      "\n",
      "Done. Paste each Week 7 query string into its function field.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nish/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-06. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# WEEK 7 QUERY GENERATOR (Weeks 1–6 data)\n",
    "# Strategy:\n",
    "#   - Exploration:                F1\n",
    "#   - Basic ML (Ridge + tuning):  F2, F3, F4, F8\n",
    "#   - Advanced ML (BO / GP-EI):   F5, F7   <-- UPDATED to trust-region BO + stabilised GP fit\n",
    "#   - Manual / Exploration-lite:  F6\n",
    "#\n",
    "# Objective direction assumed: MAXIMISE y for each function.\n",
    "# ============================================================\n",
    "\n",
    "# --- sklearn availability checks ---\n",
    "try:\n",
    "    from sklearn.linear_model import Ridge\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
    "    SKLEARN_GP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_GP_AVAILABLE = False\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) DATA: Weeks 1–6 (X inputs, y outputs) for F1..F8\n",
    "# ============================================================\n",
    "DATA = {\n",
    "    \"F1\": {\n",
    "        \"X\": np.array([\n",
    "            [0.145000, 0.515000],  # W1\n",
    "            [0.725000, 0.285000],  # W2\n",
    "            [0.515000, 0.515000],  # W3\n",
    "            [0.750000, 0.750000],  # W4\n",
    "            [0.990000, 0.010000],  # W5\n",
    "            [0.000029, 0.001417],  # W6\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            -3.353165630322361e-61,\n",
    "            6.743225602289377e-78,\n",
    "            4.714509345171323e-13,\n",
    "            1.3319145509281447e-22,\n",
    "            0.0,\n",
    "            1.825040909472812e-247,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F2\": {\n",
    "        \"X\": np.array([\n",
    "            [0.755000, 0.275000],  # W1\n",
    "            [0.785000, 0.305000],  # W2\n",
    "            [0.740000, 0.260000],  # W3\n",
    "            [0.730000, 0.270000],  # W4\n",
    "            [0.718763, 0.261649],  # W5\n",
    "            [0.722018, 0.263976],  # W6\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            0.42044085041824825,\n",
    "            -0.0456643112924181,\n",
    "            0.46274019045813003,\n",
    "            0.6060955609811236,\n",
    "            0.5195146975906033,\n",
    "            0.5794253005452772,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F3\": {\n",
    "        \"X\": np.array([\n",
    "            [0.395000, 0.875000, 0.635000],  # W1\n",
    "            [0.145000, 0.395000, 0.915000],  # W2\n",
    "            [0.120000, 0.347000, 0.943000],  # W3\n",
    "            [0.155000, 0.385000, 0.905000],  # W4\n",
    "            [0.165000, 0.375000, 0.895000],  # W5\n",
    "            [0.178771, 0.372140, 0.880781],  # W6\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            -0.12080733985523133,\n",
    "            -0.11535196594300248,\n",
    "            -0.20076336857175398,\n",
    "            -0.07852077254038155,\n",
    "            -0.06033571734237718,\n",
    "            -0.04739292498526722,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F4\": {\n",
    "        \"X\": np.array([\n",
    "            [0.275000, 0.955000, 0.515000, 0.145000],  # W1\n",
    "            [0.815000, 0.245000, 0.355000, 0.695000],  # W2\n",
    "            [0.869000, 0.174000, 0.339000, 0.750000],  # W3\n",
    "            [0.795000, 0.265000, 0.365000, 0.665000],  # W4\n",
    "            [0.785000, 0.275000, 0.370000, 0.650000],  # W5 (best)\n",
    "            [0.792676, 0.264502, 0.367988, 0.657198],  # W6\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            -18.59723490448631,\n",
    "            -14.395540985679897,\n",
    "            -18.67377341401988,\n",
    "            -13.169944884454413,\n",
    "            -12.699964227491282,\n",
    "            -12.987699814058924,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F5\": {\n",
    "        \"X\": np.array([\n",
    "            [0.635000, 0.395000, 0.755000, 0.875000],  # W1\n",
    "            [0.665000, 0.365000, 0.785000, 0.845000],  # W2\n",
    "            [0.680000, 0.350000, 0.800000, 0.830000],  # W3\n",
    "            [0.695000, 0.335000, 0.815000, 0.815000],  # W4\n",
    "            [0.707000, 0.323000, 0.827000, 0.803000],  # W5\n",
    "            [0.728000, 0.302000, 0.848000, 0.782000],  # W6 (best)\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            287.4343816627659,\n",
    "            292.2593658119571,\n",
    "            301.5311905557768,\n",
    "            315.65049985154724,\n",
    "            330.66611638919255,\n",
    "            365.66328225833024,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F6\": {\n",
    "        \"X\": np.array([\n",
    "            [0.515000, 0.145000, 0.955000, 0.395000, 0.755000],  # W1\n",
    "            [0.185000, 0.745000, 0.315000, 0.865000, 0.455000],  # W2\n",
    "            [0.152000, 0.805000, 0.251000, 0.912000, 0.425000],  # W3\n",
    "            [0.170000, 0.760000, 0.300000, 0.890000, 0.470000],  # W4\n",
    "            [0.200000, 0.730000, 0.330000, 0.840000, 0.455000],  # W5\n",
    "            [0.218000, 0.712000, 0.348000, 0.810000, 0.446000],  # W6 (best)\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            -1.6304531811460896,\n",
    "            -1.4347679755670883,\n",
    "            -1.6451191179236977,\n",
    "            -1.6022183821509282,\n",
    "            -1.3295280103104827,\n",
    "            -1.2429202946292475,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F7\": {\n",
    "        \"X\": np.array([\n",
    "            [0.875000, 0.275000, 0.635000, 0.515000, 0.145000, 0.955000],  # W1\n",
    "            [0.845000, 0.305000, 0.665000, 0.485000, 0.175000, 0.925000],  # W2\n",
    "            [0.830000, 0.320000, 0.680000, 0.470000, 0.190000, 0.910000],  # W3\n",
    "            [0.815000, 0.335000, 0.695000, 0.455000, 0.205000, 0.895000],  # W4\n",
    "            [0.805202, 0.344798, 0.704798, 0.445202, 0.214798, 0.885202],  # W5\n",
    "            [0.791730, 0.358270, 0.718270, 0.431730, 0.228270, 0.871730],  # W6 (best)\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            0.6267064847700778,\n",
    "            0.8069621926499697,\n",
    "            0.8919314248129555,\n",
    "            0.969339703275594,\n",
    "            1.0144420450032012,\n",
    "            1.0679017392374972,\n",
    "        ], float),\n",
    "    },\n",
    "\n",
    "    \"F8\": {\n",
    "        \"X\": np.array([\n",
    "            [0.145000, 0.275000, 0.395000, 0.515000, 0.635000, 0.755000, 0.875000, 0.955000],  # W1\n",
    "            [0.175000, 0.305000, 0.425000, 0.545000, 0.665000, 0.785000, 0.905000, 0.945000],  # W2\n",
    "            [0.130000, 0.260000, 0.380000, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W3\n",
    "            [0.140000, 0.270000, 0.390000, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W4\n",
    "            [0.120000, 0.250000, 0.370000, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W5\n",
    "            [0.114226, 0.244226, 0.364226, 0.500000, 0.620000, 0.740000, 0.860000, 0.960000],  # W6 (best)\n",
    "        ], float),\n",
    "        \"y\": np.array([\n",
    "            8.633935,\n",
    "            8.451335,\n",
    "            8.71814,\n",
    "            8.69914,\n",
    "            8.73594,\n",
    "            8.745671245544,\n",
    "        ], float),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) General helpers (portal formatting / clipping)\n",
    "# ============================================================\n",
    "def clip_01(x):\n",
    "    # clip to [0, 0.999999] to ensure portal \"starts with 0\"\n",
    "    return np.clip(np.asarray(x, float), 0.0, 0.999999)\n",
    "\n",
    "def format_query(x):\n",
    "    return \"-\".join(f\"{v:.6f}\" for v in np.asarray(x, float))\n",
    "\n",
    "def min_dist_to_existing(x, X_existing):\n",
    "    X_existing = np.asarray(X_existing, float)\n",
    "    d = np.sqrt(((X_existing - x)**2).sum(axis=1))\n",
    "    return float(d.min())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) (Exploration) F1: maximin random (space-filling)\n",
    "# ============================================================\n",
    "def propose_F1_maximin(X_existing, n_rand=60000, seed=11):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = X_existing.shape[1]\n",
    "    R = rng.random((n_rand, d)) * 0.999999\n",
    "    # maximin: maximise distance to closest existing point\n",
    "    dists = np.sqrt(((R[:, None, :] - X_existing[None, :, :]) ** 2).sum(axis=2))\n",
    "    min_d = dists.min(axis=1)\n",
    "    return clip_01(R[int(np.argmax(min_d))])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) (Manual) F6: continue last direction (damped)\n",
    "# ============================================================\n",
    "def propose_F6_continue_direction(X, y, gamma=0.6):\n",
    "    X = np.asarray(X, float); y = np.asarray(y, float)\n",
    "    x_prev, x_last = X[-2].copy(), X[-1].copy()\n",
    "    y_prev, y_last = float(y[-2]), float(y[-1])\n",
    "    delta = x_last - x_prev\n",
    "    # if improved -> keep direction; else reverse\n",
    "    x_new = x_last + gamma * delta if y_last >= y_prev else x_last - gamma * delta\n",
    "    return clip_01(x_new)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) (Basic ML) Ridge for F2, F3, F4, F8\n",
    "#    - tune alpha via LOOCV\n",
    "#    - tune step via step-grid + candidate search around best observed\n",
    "# ============================================================\n",
    "ALPHAS = [1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n",
    "\n",
    "STEP_GRID = {\n",
    "    \"F2\": [0.003, 0.006, 0.009, 0.012],   # sensitive near optimum\n",
    "    \"F3\": [0.008, 0.012, 0.016, 0.020],\n",
    "    \"F4\": [0.004, 0.007, 0.010, 0.013],   # narrow basin\n",
    "    \"F8\": [0.002, 0.004, 0.006, 0.008],   # ridge-tracking, tiny steps\n",
    "}\n",
    "\n",
    "# For F8, only adjust first 3 dimensions (this matched your success in W6)\n",
    "MASK = {\"F8\": [0, 1, 2]}\n",
    "\n",
    "def ridge_fit(X, y, alpha):\n",
    "    X = np.asarray(X, float); y = np.asarray(y, float)\n",
    "\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        # closed-form ridge with intercept not regularised\n",
    "        n, d = X.shape\n",
    "        Z = np.hstack([np.ones((n, 1)), X])\n",
    "        I = np.eye(d + 1); I[0, 0] = 0.0\n",
    "        beta = np.linalg.solve(Z.T @ Z + alpha * I, Z.T @ y)\n",
    "        b0 = float(beta[0]); b = beta[1:].astype(float)\n",
    "        return b0, b\n",
    "\n",
    "    model = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    model.fit(X, y)\n",
    "    return float(model.intercept_), model.coef_.astype(float)\n",
    "\n",
    "def ridge_predict(b0, b, X):\n",
    "    return b0 + np.asarray(X, float) @ np.asarray(b, float)\n",
    "\n",
    "def choose_alpha_loocv(X, y, alphas):\n",
    "    X = np.asarray(X, float); y = np.asarray(y, float)\n",
    "    n = X.shape[0]\n",
    "    best_a, best_mse = None, np.inf\n",
    "    for a in alphas:\n",
    "        errs = []\n",
    "        for i in range(n):\n",
    "            m = np.ones(n, dtype=bool); m[i] = False\n",
    "            b0, b = ridge_fit(X[m], y[m], alpha=a)\n",
    "            yhat = ridge_predict(b0, b, X[~m])[0]\n",
    "            errs.append((y[~m][0] - yhat)**2)\n",
    "        mse = float(np.mean(errs))\n",
    "        if mse < best_mse:\n",
    "            best_mse, best_a = mse, a\n",
    "    return best_a, best_mse\n",
    "\n",
    "def apply_mask(v, mask):\n",
    "    v = np.asarray(v, float)\n",
    "    if mask is None:\n",
    "        return v\n",
    "    out = np.zeros_like(v)\n",
    "    out[mask] = v[mask]\n",
    "    return out\n",
    "\n",
    "def ridge_candidates(x_best, b, step, mask=None, mode=\"local\"):\n",
    "    x_best = np.asarray(x_best, float)\n",
    "    b_eff = apply_mask(b, mask)\n",
    "    norm = np.linalg.norm(b_eff)\n",
    "\n",
    "    cands = []\n",
    "\n",
    "    # If direction is weak -> coordinate perturbations only\n",
    "    if norm < 1e-12:\n",
    "        for j in range(x_best.size):\n",
    "            if mask is not None and j not in mask:\n",
    "                continue\n",
    "            for s in [step, 0.5*step]:\n",
    "                x1 = x_best.copy(); x1[j] += s; cands.append(x1)\n",
    "                x2 = x_best.copy(); x2[j] -= s; cands.append(x2)\n",
    "        return [clip_01(c) for c in cands]\n",
    "\n",
    "    direction = b_eff / norm\n",
    "    mults = [0.5, 1.0] if mode == \"local\" else [0.5, 1.0, 1.5]\n",
    "\n",
    "    for m in mults:\n",
    "        cands.append(x_best + (m*step)*direction)\n",
    "\n",
    "    # coordinate-wise sign(b) steps (safe refinement)\n",
    "    for j in range(x_best.size):\n",
    "        if mask is not None and j not in mask:\n",
    "            continue\n",
    "        sgn = np.sign(b_eff[j])\n",
    "        if sgn == 0:\n",
    "            continue\n",
    "        xj = x_best.copy()\n",
    "        xj[j] += 0.8*step*sgn\n",
    "        cands.append(xj)\n",
    "\n",
    "    return [clip_01(c) for c in cands]\n",
    "\n",
    "def propose_ridge_next(fname, X, y, step_grid):\n",
    "    # anchor at best observed (max y)\n",
    "    best_idx = int(np.argmax(y))\n",
    "    x_best = X[best_idx].copy()\n",
    "    y_best = float(y[best_idx])\n",
    "\n",
    "    # standardise y for stability\n",
    "    y_mean, y_std = float(np.mean(y)), float(np.std(y))\n",
    "    if y_std < 1e-12:\n",
    "        y_std = 1.0\n",
    "    y_z = (y - y_mean) / y_std\n",
    "\n",
    "    # tune alpha via LOOCV\n",
    "    alpha, loocv_mse = choose_alpha_loocv(X, y_z, ALPHAS)\n",
    "    b0, b = ridge_fit(X, y_z, alpha=alpha)\n",
    "\n",
    "    mask = MASK.get(fname, None)\n",
    "\n",
    "    # tune step by scoring candidates with predicted y_z (pure exploitation)\n",
    "    best_score = -np.inf\n",
    "    best_step = None\n",
    "    best_x = None\n",
    "\n",
    "    for step in step_grid:\n",
    "        cands = ridge_candidates(x_best, b, step, mask=mask, mode=\"local\")\n",
    "        preds = ridge_predict(b0, b, np.array(cands))\n",
    "        idx = int(np.argmax(preds))\n",
    "        if float(preds[idx]) > best_score:\n",
    "            best_score = float(preds[idx])\n",
    "            best_step = step\n",
    "            best_x = cands[idx]\n",
    "\n",
    "    return {\n",
    "        \"x_next\": best_x,\n",
    "        \"alpha\": alpha,\n",
    "        \"loocv_mse\": loocv_mse,\n",
    "        \"step\": best_step,\n",
    "        \"x_best\": x_best,\n",
    "        \"y_best\": y_best,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) (Advanced ML / BO-style) UPDATED:\n",
    "#     Trust-region BO with GP + EI (stable)\n",
    "#     - standardise y manually\n",
    "#     - noise lower bound relaxed (avoid hitting 1e-10)\n",
    "#     - more local candidates near best point (strong exploitation)\n",
    "#     - smaller xi (more exploitation)\n",
    "#     - increase lbfgs iterations (reduce convergence warnings)\n",
    "# ============================================================\n",
    "def stdnorm_pdf(z):\n",
    "    return np.exp(-0.5*z*z) / math.sqrt(2.0*math.pi)\n",
    "\n",
    "def stdnorm_cdf(z):\n",
    "    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "\n",
    "def expected_improvement(mu, sigma, y_best, xi=1e-6):\n",
    "    mu = np.asarray(mu, float)\n",
    "    sigma = np.asarray(sigma, float)\n",
    "    ei = np.zeros_like(mu)\n",
    "\n",
    "    mask = sigma > 1e-12\n",
    "    imp = mu[mask] - y_best - xi\n",
    "    Z = imp / sigma[mask]\n",
    "\n",
    "    Phi = np.array([stdnorm_cdf(float(z)) for z in Z])\n",
    "    phi = stdnorm_pdf(Z)\n",
    "\n",
    "    ei[mask] = imp * Phi + sigma[mask] * phi\n",
    "    ei[ei < 0] = 0.0\n",
    "    return ei\n",
    "\n",
    "def fit_gp_stable(X, y, seed=0):\n",
    "    if not SKLEARN_GP_AVAILABLE:\n",
    "        raise ImportError(\"scikit-learn GaussianProcessRegressor is not available.\")\n",
    "\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "    d = X.shape[1]\n",
    "\n",
    "    kernel = (\n",
    "        C(1.0, (1e-2, 1e3)) *\n",
    "        Matern(length_scale=np.ones(d), length_scale_bounds=(1e-2, 10.0), nu=2.5)\n",
    "        + WhiteKernel(noise_level=1e-4, noise_level_bounds=(1e-6, 1e-1))\n",
    "    )\n",
    "\n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        normalize_y=False,           # we standardise y ourselves\n",
    "        n_restarts_optimizer=12,\n",
    "        random_state=seed,\n",
    "        optimizer=\"fmin_l_bfgs_b\",\n",
    "    )\n",
    "\n",
    "    # Increase optimizer iterations to reduce lbfgs warnings\n",
    "    gp._optimizer_kwargs = {\"maxiter\": 5000}\n",
    "\n",
    "    gp.fit(X, y)\n",
    "    return gp\n",
    "\n",
    "def propose_bo_trust_region(fname, X, y, seed=0, xi=1e-6, n_local=30000, n_global=2000, local_sigma=0.02):\n",
    "    \"\"\"\n",
    "    Trust-region BO:\n",
    "      - Fit GP on standardised y\n",
    "      - Sample mostly locally around x_best (strong exploitation)\n",
    "      - Keep small global sample set to avoid pathological traps\n",
    "      - Select point that maximises EI\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float); y = np.asarray(y, float)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    best_idx = int(np.argmax(y))\n",
    "    x_best = X[best_idx].copy()\n",
    "    y_best = float(y[best_idx])\n",
    "\n",
    "    # Standardise y for numerical stability\n",
    "    y_mean = float(np.mean(y))\n",
    "    y_std = float(np.std(y))\n",
    "    if y_std < 1e-12:\n",
    "        y_std = 1.0\n",
    "    y_z = (y - y_mean) / y_std\n",
    "    y_best_z = float(np.max(y_z))\n",
    "\n",
    "    gp = fit_gp_stable(X, y_z, seed=seed)\n",
    "\n",
    "    d = X.shape[1]\n",
    "\n",
    "    # local candidates\n",
    "    L = x_best + rng.normal(0.0, local_sigma, size=(n_local, d))\n",
    "    L = clip_01(L)\n",
    "\n",
    "    # global candidates\n",
    "    G = rng.random((n_global, d)) * 0.999999\n",
    "\n",
    "    CANDS = np.vstack([L, G])\n",
    "\n",
    "    # filter out duplicates / extremely close points\n",
    "    keep = []\n",
    "    for i in range(CANDS.shape[0]):\n",
    "        keep.append(min_dist_to_existing(CANDS[i], X) > 1e-6)\n",
    "    CANDS = CANDS[np.array(keep, dtype=bool)]\n",
    "    if CANDS.shape[0] == 0:\n",
    "        CANDS = rng.random((3000, d)) * 0.999999\n",
    "\n",
    "    mu, sigma = gp.predict(CANDS, return_std=True)\n",
    "    ei = expected_improvement(mu, sigma, y_best=y_best_z, xi=xi)\n",
    "\n",
    "    idx = int(np.argmax(ei))\n",
    "    x_next = clip_01(CANDS[idx])\n",
    "\n",
    "    return {\n",
    "        \"x_next\": x_next,\n",
    "        \"x_best\": x_best,\n",
    "        \"y_best\": y_best,\n",
    "        \"kernel_\": str(gp.kernel_),\n",
    "        \"xi\": xi,\n",
    "        \"local_sigma\": local_sigma,\n",
    "        \"n_candidates\": int(CANDS.shape[0]),\n",
    "        \"best_ei\": float(ei[idx]),\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Build the Week 7 plan using your requested mapping\n",
    "# ============================================================\n",
    "PLAN = {}\n",
    "\n",
    "# Exploration: F1\n",
    "PLAN[\"F1\"] = (\"EXPLORATION_MAXIMIN\", {\"x_next\": propose_F1_maximin(DATA[\"F1\"][\"X\"], n_rand=60000, seed=11)})\n",
    "\n",
    "# Basic ML ridge: F2, F3, F4, F8\n",
    "for f in [\"F2\", \"F3\", \"F4\", \"F8\"]:\n",
    "    PLAN[f] = (\"BASIC_ML_RIDGE_TUNED\", propose_ridge_next(f, DATA[f][\"X\"], DATA[f][\"y\"], step_grid=STEP_GRID[f]))\n",
    "\n",
    "# Advanced ML BO-style (UPDATED): F5, F7\n",
    "PLAN[\"F5\"] = (\"ADV_ML_BO_GP_EI_TR\", propose_bo_trust_region(\"F5\", DATA[\"F5\"][\"X\"], DATA[\"F5\"][\"y\"], seed=21, xi=1e-6, local_sigma=0.02))\n",
    "PLAN[\"F7\"] = (\"ADV_ML_BO_GP_EI_TR\", propose_bo_trust_region(\"F7\", DATA[\"F7\"][\"X\"], DATA[\"F7\"][\"y\"], seed=22, xi=1e-6, local_sigma=0.02))\n",
    "\n",
    "# Manual/exploration-lite: F6\n",
    "PLAN[\"F6\"] = (\"MANUAL_CONTINUE_DIR\", {\"x_next\": propose_F6_continue_direction(DATA[\"F6\"][\"X\"], DATA[\"F6\"][\"y\"], gamma=0.60)})\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) Print Week 7 queries for portal (F1..F8)\n",
    "# ============================================================\n",
    "print(\"==== WEEK 7 QUERIES (COPY/PASTE INTO PORTAL) ====\\n\")\n",
    "\n",
    "for f in [\"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"F6\",\"F7\",\"F8\"]:\n",
    "    method, info = PLAN[f]\n",
    "    x_next = info[\"x_next\"]\n",
    "\n",
    "    print(f\"{f}  [{method}]\")\n",
    "    print(f\"  Week 7 query: {format_query(x_next)}\")\n",
    "\n",
    "    # Print tuning details so you can use them in Week 7 reflection\n",
    "    if method == \"BASIC_ML_RIDGE_TUNED\":\n",
    "        print(f\"  tuned alpha: {info['alpha']} | LOOCV MSE: {info['loocv_mse']:.6f} | tuned step: {info['step']}\")\n",
    "        print(f\"  best observed y: {info['y_best']:.6f} at x_best={format_query(info['x_best'])}\")\n",
    "\n",
    "    if method == \"ADV_ML_BO_GP_EI_TR\":\n",
    "        print(f\"  GP kernel: {info['kernel_']}\")\n",
    "        print(f\"  EI xi: {info['xi']} | local_sigma: {info['local_sigma']} | candidates: {info['n_candidates']} | best EI: {info['best_ei']:.6e}\")\n",
    "        print(f\"  best observed y: {info['y_best']:.6f} at x_best={format_query(info['x_best'])}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print(\"Done. Paste each Week 7 query string into its function field.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c9071-8246-4821-82b2-72e7d023cc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
